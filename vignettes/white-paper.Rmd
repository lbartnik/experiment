---
title: "Supporting Data Processing with an Object Repository"
author: "Lukasz A. Bartnik"
date: "`r Sys.Date()`"
output: rmarkdown::pdf_document
vignette: >
  %\VignetteIndexEntry{Supporting Data Processing with an Object Repository}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Abstract

Processing data in R requires a certain level of diligence in tracking
the progress of work, which is necessary for a number of reasons. For
example, without some form of organizing principle, a body of work built
over days or weeks can become unmanageable and thus thwart, or at least
profoundly slow down, any further progress. Similarily, sharing source
results with third parties, a foundational principle of _reproducible
science_[^reproducible-research], can be rendered virtually impossible if the relevant data
set or code cannot be successfully identified. There is already a
multitude of means and aids instrumental in achieving this goal, and
users of R and other software packages tend to find their own, personal
choice of such means. This paper proposes another tool, one that has the
ambition, however, to fill certain gaps that seems to have been so far
left unaddressed. This new tool is an interactive repository of objects,
code and plots, which is built without any disturbance to the user along
with his or her work with data, in R. Whenever a certain detail needs
to be brought back from the past and consulted, the proposed repository
delivers just that by the means of an elaborate search interface.
Finally, user can choose to continue using their own specific set of
tools and augment their work only when necessary.


# Introduction

R comes with a very straighforward computing model: the interactive R
session which holds a number of data objects and code. Data objects
are processed in an interative manner, leading from the original data
set, to intermediate data objects and plots, to final deliverables,
which typically are reports and models, predictive or descriptive.
The R programming language and runtime are designed to streamline this
process, and so are numerous R extension packages. There is also a variety
of external tools and practices external to R meant to streamline data
analysis in R, which itself consists of tasks that fall into one of the
following categories.

The first category is producing and keeping track of the research
artifacts: data, code, models, plots, etc.

saving data and code in the filesystem; git; `archivist`; other DBs


The second category has to do with turning the research artifacts into
deliverables: reports and models.

`knitr`, `rmarkdown`, other doc formats; RData with the actual model



The third category is organizing research artifacts and deliverables
into _reproducible research_.

again, `knitr`; `archivist`




R combined with CRAN, its enormous library of extension packages, are
an amazing tool for data analysis and research. Among them, there are
a number of packages whose purpose is simplifying the general task of
scientific research and organizing the knowledge collected in that
research. The largest group of those implements certain aspects of
_literate programming_, for example `knitr`, `rmarkdown`, `markdown`,
the `Sweave` function in the base `utils`. Other packages implement
format conversion (`pander`), object caching (`archivist` and `R.cache`)
or package caching (`packrat`, `checkpoint`, `miniCRAN`). More categories
and packages are listed in the _Reproducible Research_ view[^rr-view] on
CRAN.

[^rr-view]: https://CRAN.R-project.org/view=ReproducibleResearch

Even though these package simplify a great deal of tasks caried out
typically in R, they do not remove the overhead typical when working
with R entirely.

there is still a broad spectrum of work required in
order to conduct successful research. These include 



is done and knowledge 


object browser - repository search - repository of objects - R session state - history of R session state 





extended abstract

summary of sections that follow



About this course: This course focuses on the concepts and tools behind
reporting modern data analyses in a reproducible manner. Reproducible
research is the idea that data analyses, and more generally, scientific
claims, are published with their data and software code so that others
may verify the findings and build upon them.  The need for reproducibility
is increasing dramatically as data analyses become more complex,
involving larger datasets and more sophisticated computations.
Reproducibility allows for people to focus on the actual content of a
data analysis, rather than on superficial details reported in a written
summary. In addition, reproducibility makes an analysis more useful to
others because the data and code that actually conducted the analysis
are available. This course will focus on literate statistical analysis
tools which allow one to publish data analyses in a single document
that allows others to easily execute the same analysis to obtain the
same results.




## Popular R Packages and Extensions

these serve as a form of cache memory - file-system-based, if needed;
but it is always a working memory, one that is intensely used, re-written
and changed - it is the state of the current research project


R Markdown - combine code, data and visualizations in order to produce
a reproducible report; downside: a lot of effort, not so much in-line
with interactive work typical for R

archivist - store artifacts and use their checksums; preserves the
actual data relevant to the decision (plot and data set itself), but
does not preserve the relationships between objects, code and plots

git - store and version code (and data); requires extra effort in order
to verify whether the commit was in fact self-consistent, that is,
whether data match the code; in retrospect, after a few weeks/months,
if code does not reproduce data, there is always a suspicion the commit
itself had been made erroneously


# Working with Data

loop: generate hypothesis, verify hypothesis

generates artifacts: data objects (data sets, models, summaries), code
(ETL functions, model formulas) and plots (but also tables, and others
visualizations, e.g. interactive)

similar to dev loop: https://leankit.com/learn/agile/agile-software-development/

generating and verifying is a high-intensity task, documenting the
work becomes the dreaded but necessary overhead

as with software, one needs to plan where they want to go and document
the road so far, so that their deliverables constitue a body of work
intelligible for themselves as well as for third parties; which include:
peers (reviewers, scientific/academic papers), peers and superiors
(business reports), general public (blog posts)

we need to run all of our work against a cache memory, where the state
of the research is written to; it can be (as mentioned) a document,
an actual object repository or a bunch of files

without an adequate effort, the number of artifacts (and their versions)
can grow and push the complexity of the project beyond the level of
manageability; so the cache memory needs to be not only written to,
but also constantly re-organized, to constitute a representation of
what we know and think about the project

it is when wrong versions of data sets or code are being delivered
(sent over to production/implementation, reported to decision-makers,
etc.), reused (for similar projects, or if a report is required in
a cycle, e.g. monthly), or used a basis for further investigations
(e.g. grouping model is used to segment data, and then regression
models are built separately for each segment)

such mistakes in the long run are inevitable but the person conducting
the research should always strive to minimize the risk of making one

classification of mistakes: data (model), code (controller), plot (view)

data      | code      | plot      | example
correct   | incorrect | -         | ETL procedure is applied to a sample and then to the whole set
incorrect | correct   | -         | code is discarded, time is wasted
correct   | correct   | incorrect | good reults are discarded, wrong ETL code is used
... ... ...

view - decide to keep or discard code/data
data - report wrong results
code - apply incorrect procedure to another data set; stop other from reproducing results


# Problems

these problems are human-specific, that is, they stem from out limited human
nature; there are various ways to classify them, but not really a formal way
to define them


identifying a single object - we know it's there, but we cannot find it;
maybe it was a part of a report (a few rows from a larger table), or there
is a plot that points to it (a table or a model), maybe there is come
characteristic of a model (AIC, R^2) but we cannot find the model that
actually matches it; in all of these cases being able to look at *all*
objects produced so far during the research project will give the definite
answer;
here we can be saving time (if the object is somewhere) or the reproducibility
itself (if for some reason we don't have it anymore)


verifying results so far - building a solution for a data-related problem
sometimes requires a number of iterations where various models (structure=formula
and kind lm vs. glm vs. tree) are generated, verified and finally compared;
those models need to be stored somewhere, be it RData files, RMarkdown document
or some other form of cache/storage;
because keeping record of all of those artifact can be confusing (?how? which
model did I write down, which are still in R's memory?) and introduce "a lot"
of extra overhead, we tend to limit what we write down only what we think will
be relevant in the end; having an intelligent cache that records everything
and then lets us pick and choose what we want to see and compare can be time-saving;
here we save time, most definitely, but also reduce the likelihood of errors
of omission, e.g. when part of the work disappears from the final report
because the user couldn't find it or forgot where it was stored


looking at the big picture - what have we learned so far, what do we think
and what do we know about the research project; what do we know about the
data, how do we want to approach the remainder of the work



# Repository - Model

store data, code and plots, record state of R session after each command,
together with all relevant details (e.g. packages and their versions, the
command itself); a.k.a "commit"

preserve the order of commits, effectively representing the whole history
of one's work

store objects under their SHA1, identify repetitions

allow to "go back in time" (retrieve a historical commit) and branch off
in that commit, thus preserving even more information about the
relationships between artifacts in the repository



# Repository vs. Problems

## Search for a Single Object

examples of questions that need to be answered and what kind of queries
return the requested information; a single object, a plot related to an
object, etc.

artifacts stored in R's global environment under the name of `m`

```{r eval=FALSE}
search(objects, named("m"))
```


artifacts that match some specific contents

```{r eval=FALSE}
search(objects, has_names("cylinder", "diameter"), has_rows(c(1, 5.5), c(10, 12.3)))
```




## Search for a Group of Objects

objects related to some shared source - e.g. various models derived from
the same original data set (directly or via other ETL-ed data sets); e.g.
inheriting from a given class, etc.


linear models built from input

```{r eval=FALSE}
search(objects, inherits("lm"), derived("input"))
```


any model derived from this specific data set

```{r eval=FALSE}
search(objects, is_model, derived(iris))
```


any model derived directly from this data set (no ETL expressions
that produce intermediate data set, unless they are a part of the
expression that produced that given model)

```{r eval=FALSE}
search(objects, inherits("lm"), derived(iris, direct = TRUE))
```


## Overview in Browser

when exploring (as opposed to "exploiting") the "search space", browser
can help build a high-level "map" of work done so far, various branches
in the timeline of the repository

logical grouping of objects - user can assign names/groups/branches to
subsets of the artifact tree and enrich the informations available to
himself (at a later time) or to his collaborators


also, when the id of the object is reported, and access to the object
store (repository) is granted, other researchers can investigate the
actual steps that led to creation of the said final result (object)



# Summary



# References

 * https://leankit.com/learn/agile/agile-software-development/
 * https://www.jvcasillas.com/post/2015-05-18_data_pipelines/
 * https://drsimonj.svbtle.com/a-tidy-model-pipeline-with-twidlr-and-broom
 * https://www.rstudio.com/resources/webinars/pipelines-for-data-analysis-in-r/
 * https://rstudio-pubs-static.s3.amazonaws.com/227736_57d977300b254865b95c256860d27209.html
 * https://cran.r-project.org/web/packages/pipeliner/README.html
 * https://cran.r-project.org/web/views/ReproducibleResearch.html
 * https://www.coursera.org/learn/reproducible-research
 * https://en.wikipedia.org/wiki/Reproducibility
 * https://www.r-bloggers.com/what-is-reproducible-research/

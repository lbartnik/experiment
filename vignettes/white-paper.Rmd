---
title: "Supporting Data Processing with an Object Repository"
author: "Lukasz A. Bartnik"
date: "`r Sys.Date()`"
output: rmarkdown::pdf_document
vignette: >
  %\VignetteIndexEntry{Supporting Data Processing with an Object Repository}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Abstract

Processing data in R requires a certain level of diligence in tracking
the progress of work, which is necessary for a number of reasons. For
example, without some form of organizing principle, a body of work built
over days or weeks can become unmanageable and thus thwart, or at least
profoundly slow down, any further progress. Similarily, sharing source
results with third parties, a foundational principle of _reproducible
science_[^reproducible-research], can be rendered virtually impossible if the relevant data
set or code cannot be successfully identified. There is already a
multitude of means and aids instrumental in achieving this goal, and
users of R and other software packages tend to find their own, personal
choice of such means. This paper proposes another tool, one that has the
ambition, however, to fill certain gaps that seems to have been so far
left unaddressed. This new tool is an interactive repository of objects,
code and plots, which is built without any disturbance to the user along
with his or her work with data, in R. Whenever a certain detail needs
to be brought back from the past and consulted, the proposed repository
delivers just that by the means of an elaborate search interface.
Finally, user can choose to continue using their own specific set of
tools and augment their work only when necessary.


# Introduction

R comes with a very straighforward computing model: the interactive R
session holds a number of data and code objects and its contents are
itertively changed according to commands issued by the user. This is how
the original data are transformed into intermediate data objects and
plots, and then into final deliverables, which typically are reports
and models. The R programming language and runtime are designed
specifically to streamline this process, and so are numerous R extension
packages available on CRAN and elsewhere. There is also a variety
of tools external to R which combined constitute a _data science ecosystem_.
The process of data analysis in this ecosystem can be characterized by
the following three categories of activities and specific tools.

The first category encompasses means of maintaining the state of the project,
that is, numerous artifacts like data, code, models, plots, which is
different from the state of the current iteration held by the R session.
Since this task bears some similarity to software developments, the
available tools and conventions resemble those known from the world of
software development. Typically, the state of the projects is represented
in the filesystem and is organized and managed by:
conventions like hierarchies of files in the filesystem (e.g. projects
in `RStudio`), open and simple file formats (e.g. `Markdown` and `HTML`,
`CSV` and `RData`), or tools like `git` and DBMSes.



The second category has to do with turning the research artifacts into
deliverables: reports and models.
`knitr`, `rmarkdown`, other doc formats; RData with the actual model

The third category is organizing research artifacts and deliverables
into _reproducible research_; again, `knitr`; also `archivist`

Categories two and three:
The largest group of those implements certain aspects of
_literate programming_, for example `knitr`, `rmarkdown`, `markdown`,
the `Sweave` function in the base `utils`.
package caching (`packrat`, `checkpoint`, `miniCRAN`). More categories
and packages are listed in the _Reproducible Research_ view[^rr-view] on
CRAN.

[^rr-view]: https://CRAN.R-project.org/view=ReproducibleResearch

maybe to section 2 (popular packages)?




All of these tools aim at reducing the overhead of conducting research
in R which is imminent in organizing and versioning the artifacts and
reports, documenting the process and making it reproducible. Although
over the years that overhead has been reduced to a large extent certain
aspects remain present. And since it is typical to human nature to skip
or avoid repetitive tasks, especially when they slow down the perceived
progress (thus, overhead), that leads to certain errors of ommission and
errors of commission. Examples are: mixing up the code and data (don't
know which code produces data), losing access to artifacts (don't know
where they are), entirely forgetting about artifacts (don't know that
don't know where something is).


This paper describes a proposed extension to the current toolset: an
interactive history of R session (not only commands) built on top of
a repository of R objects.


summary of sections that follow



About this course: This course focuses on the concepts and tools behind
reporting modern data analyses in a reproducible manner. Reproducible
research is the idea that data analyses, and more generally, scientific
claims, are published with their data and software code so that others
may verify the findings and build upon them.  The need for reproducibility
is increasing dramatically as data analyses become more complex,
involving larger datasets and more sophisticated computations.
Reproducibility allows for people to focus on the actual content of a
data analysis, rather than on superficial details reported in a written
summary. In addition, reproducibility makes an analysis more useful to
others because the data and code that actually conducted the analysis
are available. This course will focus on literate statistical analysis
tools which allow one to publish data analyses in a single document
that allows others to easily execute the same analysis to obtain the
same results.




## Popular R Packages and Extensions

these serve as a form of cache memory - file-system-based, if needed;
but it is always a working memory, one that is intensely used, re-written
and changed - it is the state of the current research project


R Markdown - combine code, data and visualizations in order to produce
a reproducible report; downside: a lot of effort, not so much in-line
with interactive work typical for R

archivist - store artifacts and use their checksums; preserves the
actual data relevant to the decision (plot and data set itself), but
does not preserve the relationships between objects, code and plots

git - store and version code (and data); requires extra effort in order
to verify whether the commit was in fact self-consistent, that is,
whether data match the code; in retrospect, after a few weeks/months,
if code does not reproduce data, there is always a suspicion the commit
itself had been made erroneously


# Working with Data

loop: generate hypothesis, verify hypothesis

generates artifacts: data objects (data sets, models, summaries), code
(ETL functions, model formulas) and plots (but also tables, and others
visualizations, e.g. interactive)

similar to dev loop: https://leankit.com/learn/agile/agile-software-development/

generating and verifying is a high-intensity task, documenting the
work becomes the dreaded but necessary overhead

as with software, one needs to plan where they want to go and document
the road so far, so that their deliverables constitue a body of work
intelligible for themselves as well as for third parties; which include:
peers (reviewers, scientific/academic papers), peers and superiors
(business reports), general public (blog posts)

we need to run all of our work against a cache memory, where the state
of the research is written to; it can be (as mentioned) a document,
an actual object repository or a bunch of files

without an adequate effort, the number of artifacts (and their versions)
can grow and push the complexity of the project beyond the level of
manageability; so the cache memory needs to be not only written to,
but also constantly re-organized, to constitute a representation of
what we know and think about the project

it is when wrong versions of data sets or code are being delivered
(sent over to production/implementation, reported to decision-makers,
etc.), reused (for similar projects, or if a report is required in
a cycle, e.g. monthly), or used a basis for further investigations
(e.g. grouping model is used to segment data, and then regression
models are built separately for each segment)

such mistakes in the long run are inevitable but the person conducting
the research should always strive to minimize the risk of making one

classification of mistakes: data (model), code (controller), plot (view)

data      | code      | plot      | example
correct   | incorrect | -         | ETL procedure is applied to a sample and then to the whole set
incorrect | correct   | -         | code is discarded, time is wasted
correct   | correct   | incorrect | good reults are discarded, wrong ETL code is used
... ... ...

view - decide to keep or discard code/data
data - report wrong results
code - apply incorrect procedure to another data set; stop other from reproducing results


# Problems

data and code run out-of-sync; having the repo amends that as data and code
for any given moment in history can be uniquely identified and retrieved from
the repo and relationships between artifacts written there; NICE!



these problems are human-specific, that is, they stem from out limited human
nature; there are various ways to classify them, but not really a formal way
to define them


identifying a single object - we know it's there, but we cannot find it;
maybe it was a part of a report (a few rows from a larger table), or there
is a plot that points to it (a table or a model), maybe there is come
characteristic of a model (AIC, R^2) but we cannot find the model that
actually matches it; in all of these cases being able to look at *all*
objects produced so far during the research project will give the definite
answer;
here we can be saving time (if the object is somewhere) or the reproducibility
itself (if for some reason we don't have it anymore)


verifying results so far - building a solution for a data-related problem
sometimes requires a number of iterations where various models (structure=formula
and kind lm vs. glm vs. tree) are generated, verified and finally compared;
those models need to be stored somewhere, be it RData files, RMarkdown document
or some other form of cache/storage;
because keeping record of all of those artifact can be confusing (?how? which
model did I write down, which are still in R's memory?) and introduce "a lot"
of extra overhead, we tend to limit what we write down only what we think will
be relevant in the end; having an intelligent cache that records everything
and then lets us pick and choose what we want to see and compare can be time-saving;
here we save time, most definitely, but also reduce the likelihood of errors
of omission, e.g. when part of the work disappears from the final report
because the user couldn't find it or forgot where it was stored


looking at the big picture - what have we learned so far, what do we think
and what do we know about the research project; what do we know about the
data, how do we want to approach the remainder of the work



# Repository - Model

store data, code and plots, record state of R session after each command,
together with all relevant details (e.g. packages and their versions, the
command itself); a.k.a "commit"

preserve the order of commits, effectively representing the whole history
of one's work

store objects under their SHA1, identify repetitions

allow to "go back in time" (retrieve a historical commit) and branch off
in that commit, thus preserving even more information about the
relationships between artifacts in the repository



# Repository vs. Problems

## Search for a Single Object

examples of questions that need to be answered and what kind of queries
return the requested information; a single object, a plot related to an
object, etc.

artifacts stored in R's global environment under the name of `m`

```{r eval=FALSE}
search(objects, named("m"))
```


artifacts that match some specific contents

```{r eval=FALSE}
search(objects, has_names("cylinder", "diameter"), has_rows(c(1, 5.5), c(10, 12.3)))
```




## Search for a Group of Objects

objects related to some shared source - e.g. various models derived from
the same original data set (directly or via other ETL-ed data sets); e.g.
inheriting from a given class, etc.


linear models built from input

```{r eval=FALSE}
search(objects, inherits("lm"), derived("input"))
```


any model derived from this specific data set

```{r eval=FALSE}
search(objects, is_model, derived(iris))
```


any model derived directly from this data set (no ETL expressions
that produce intermediate data set, unless they are a part of the
expression that produced that given model)

```{r eval=FALSE}
search(objects, inherits("lm"), derived(iris, direct = TRUE))
```


## Overview in Browser

when exploring (as opposed to "exploiting") the "search space", browser
can help build a high-level "map" of work done so far, various branches
in the timeline of the repository

logical grouping of objects - user can assign names/groups/branches to
subsets of the artifact tree and enrich the informations available to
himself (at a later time) or to his collaborators


also, when the id of the object is reported, and access to the object
store (repository) is granted, other researchers can investigate the
actual steps that led to creation of the said final result (object)



# Summary



# References

 * https://leankit.com/learn/agile/agile-software-development/
 * https://www.jvcasillas.com/post/2015-05-18_data_pipelines/
 * https://drsimonj.svbtle.com/a-tidy-model-pipeline-with-twidlr-and-broom
 * https://www.rstudio.com/resources/webinars/pipelines-for-data-analysis-in-r/
 * https://rstudio-pubs-static.s3.amazonaws.com/227736_57d977300b254865b95c256860d27209.html
 * https://cran.r-project.org/web/packages/pipeliner/README.html
 * https://cran.r-project.org/web/views/ReproducibleResearch.html
 * https://www.coursera.org/learn/reproducible-research
 * https://en.wikipedia.org/wiki/Reproducibility
 * https://www.r-bloggers.com/what-is-reproducible-research/

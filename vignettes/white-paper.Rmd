---
title: "Supporting Data Processing with an Object Repository"
author: "Lukasz A. Bartnik"
date: "`r Sys.Date()`"
output: rmarkdown::pdf_document
vignette: >
  %\VignetteIndexEntry{Supporting Data Processing with an Object Repository}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Abstract

Processing data in R requires a certain level of diligence in tracking
the progress of work, which is necessary for a number of reasons. For
example, without some form of organizing principle, a body of work built
over days or weeks can become unmanageable and thus thwart, or at least
profoundly slow down, any further progress. Similarily, sharing source
results with third parties, a foundational principle of _reproducible
science_[^reproducible-research], can be rendered virtually impossible if the relevant data
set or code cannot be successfully identified. There is already a
multitude of means and aids instrumental in achieving this goal, and
users of R and other software packages tend to find their own, personal
choice of such means. This paper proposes another tool, one that has the
ambition, however, to fill certain gaps that seems to have been so far
left unaddressed. This new tool is an interactive repository of objects,
code and plots, which is built without any disturbance to the user along
with his or her work with data, in R. Whenever a certain detail needs
to be brought back from the past and consulted, the proposed repository
delivers just that by the means of an elaborate search interface.
Finally, user can choose to continue using their own specific set of
tools and augment their work only when necessary.


# Introduction

R comes with a very straighforward computing model: the interactive R
session holds a number of data and code objects and its contents are
itertively changed according to commands issued by the user. This is how
the original data are transformed into intermediate data objects and
plots, and then into final deliverables, which typically are reports
and models. The R programming language and runtime are designed
specifically to streamline this process, and so are numerous R extension
packages available on CRAN and elsewhere. There is also a variety
of tools external to R which combined constitute a _data science ecosystem_.
The process of data analysis in this ecosystem can be characterized by
the following three categories of activities and specific tools.

The first category encompasses means of maintaining the state of the project,
that is, numerous artifacts like data, code, models, plots, which is
different from the state of the current iteration held by the R session.
Since this task bears some similarity to software developments, the
available tools and conventions resemble those known from the world of
software development. Typically, the state of the projects is represented
in the filesystem and is organized and managed by:
conventions like hierarchies of files in the filesystem (e.g. projects
in `RStudio`), open and simple file formats (e.g. `Markdown` and `HTML`,
`CSV` and `RData`), or tools like `git` and DBMSes.



The second category has to do with turning the research artifacts into
deliverables: reports and models. Gauging the level of details reported
is a task in itself, which leads to various versions of the same report
aimed at differing audiences: for business stakeholders, for other
researchers, for yourself to come back to an interrupted research project
at a later time.
`knitr`, `rmarkdown`, other doc formats; RData with the actual model


The third category is organizing research artifacts and deliverables
into _reproducible research_ which means two things:

  1. consistent results, that is, code and data match each other and
     can be re-created
  2. granting access to source data and code to others
  
again, `knitr`; also `archivist`.




Categories two and three:
The largest group of those implements certain aspects of
_literate programming_, for example `knitr`, `rmarkdown`, `markdown`,
the `Sweave` function in the base `utils`.
package caching (`packrat`, `checkpoint`, `miniCRAN`). More categories
and packages are listed in the _Reproducible Research_ view[^rr-view] on
CRAN.

[^rr-view]: https://CRAN.R-project.org/view=ReproducibleResearch

maybe to section 2 (popular packages)?




All of these tools aim at reducing the overhead of conducting research
in R which is imminent in organizing and versioning the artifacts and
reports, documenting the process and making it reproducible. Although
over the years that overhead has been reduced to a large extent certain
aspects remain present. And since it is typical to human nature to skip
or avoid repetitive tasks, especially when they slow down the perceived
progress (thus, overhead), that leads to certain errors of ommission and
errors of commission. Examples are: mixing up the code and data (don't
know which code produces data), losing access to artifacts (don't know
where they are), entirely forgetting about artifacts (don't know that
don't know where something is).


This paper describes a proposed extension to the current toolset: an
implementation of history of multiple R sessions as executed during a
research project. This particular implementation differs from the history
mechanism already present in R in that it collects not only expressions
(commands) issued by the user, but also:

  * the artifacts (data, functions, plots) these expressions produce
    with the ability to restore the state of R session at any point in
    the history
  * points in which a historical state is loaded into R session, which
    leads to "branches" in the history that form a history "tree", rather
    than the linear model of expressions present in R
  * the relationships between those artifacts: object/plot `y` is produced
    by function `f` which takes object `x` as its input

The history recorded in this way can be browsed in a dedicated graphical
tool implemented as a Shiny-based widget, or via a comprehensive
command-line R API. The Shiny-based browser is also capable of turning
a number of selected objects (data, code/functions and plots) into a
RMarkdown report.

The introductory section is followed by Section 2 with a short survey of
software packages typically considered to be a part of toolsets for data
science or reproducible research. Section 3 describes a general model of
conducting data-related research in R, followed by Section 4 with a list
of problems and challenges inherent in such research process. Section 5
introduces a commit-based model of a history or objects generated in an
interactive R session. Section 6 shows how such history can be an answer
to problems and challenges described in Section 4. Finally Section 7
presents further examples of the interactive, Shiny-based history browser
and the programmatic search API. The paper closes with a brief discussion
of the further work and outlook of the project.




From Coursera, "about this course": This course focuses on the concepts and tools behind
reporting modern data analyses in a reproducible manner. Reproducible
research is the idea that data analyses, and more generally, scientific
claims, are published with their data and software code so that others
may verify the findings and build upon them.  The need for reproducibility
is increasing dramatically as data analyses become more complex,
involving larger datasets and more sophisticated computations.
Reproducibility allows for people to focus on the actual content of a
data analysis, rather than on superficial details reported in a written
summary. In addition, reproducibility makes an analysis more useful to
others because the data and code that actually conducted the analysis
are available. This course will focus on literate statistical analysis
tools which allow one to publish data analyses in a single document
that allows others to easily execute the same analysis to obtain the
same results.




# Popular R Packages and Extensions

three categories: literate programming, object cache, software development
tools


these serve as a form of cache memory - file-system-based, if needed;
but it is always a working memory, one that is intensely used, re-written
and changed - it is the state of the current research project


R Markdown - combine code, data and visualizations in order to produce
a reproducible report; downside: a lot of effort, not so much in-line
with interactive work typical for R

archivist - store artifacts and use their checksums; preserves the
actual data relevant to the decision (plot and data set itself), but
does not preserve the relationships between objects, code and plots

git - store and version code (and data); requires extra effort in order
to verify whether the commit was in fact self-consistent, that is,
whether data match the code; in retrospect, after a few weeks/months,
if code does not reproduce data, there is always a suspicion the commit
itself had been made erroneously


# Working with Data

Introduce the broad term of data science here, as this is what we will
refer to. Use the term "data project" as a general reference point and
the point of focus; define the term "artifact" and then use it without
enumerating what can be an artifact


exploration (discovering) vs. exploitation (modelling)

a data project starts with a goal: 
Descriptive Analytics, which use data aggregation and data mining to provide insight into the past and answer: “What has happened?”
Predictive Analytics, which use statistical models and forecasts techniques to understand the future and answer: “What could happen?”
Prescriptive Analytics, which use optimization and simulation algorithms to advice on possible outcomes and answer: “What should we do?”

build a descriptive model, a predictive one, or maybe even a prescriptive
one; this leads to a number of iterations where we deepen our understanding
of the data and build increasingly sophisticated and efficient models


we start with a general overview of the data set: we clean the formats,
work out the joins, assess the missing and uncertain data and decide on
how to impute missing data and/or identify and correct uncertain (e.g
anomalies in readings); finally, we arrive at a data set that is ready
for introductory analysis; we might come back to verify the input some
more in case we find more problems during exploration or modelling; we
finish this by further exploration - obviously, to assess anomalies we
needed to understand the data enough, so there was some exporation done,
and now we finish it off by maybe choosing the subset of the data for
the first modelling phase (step)

during this work, we often generate a lot of plots, models, functions
(artifacts) that somehow, gradually reveal the knowledge about the
data set; this is when we compose a report of some kind that describes
our initial findings and explains decisions that lead to the model that
will be developed and validated during the exploitation phase
(we could probably add something to the package that prints the id
of each object generated in the RMarkdown report during its compilation);
it's nice to have it all bookkept - even if stuff is in reports, not
everything will be described accurately, esp. that building a good report
is a task in itself and adds to the overhead; so we write down the most
important things and rely on the data stored - somewhere; what if all of
that data lands in the repo together with all of the relationships between
objects and code? it's the ultimate backup



now we start the exploitation - we build a model for that subset, either
predictive (classifier, forecasting, regression) or descriptive

we build models and generate the best answer to the current question;
when we arrive at something satisfying, we move back to the bigger data
set (not the subset anymore) and verify how well our local model does on
that bigger data set; then we move back and forth, until we are satisfied
with the model formula;
during this phase we generate fewer artifacts, but more in terms of versions
of a given artifact - e.g. a model - but characterized by plots or some
concise error measure (so binary objects are good but not what we will be
looking at when browsing the repo)




now we can decide whether we learned something interesting about the data
set that justifies changing the structure of the model (extra features or
interactions) or trying an alternative model (NN instead of LM or ARIMA,
a different classifier or a different regression method, SVM instead of LM)





----






On some general level conducting research in R can be viewed as a series
of iterations in which a hypothesis is first formulated and then verified.
The process starts with stating the expected final hypothesis, the
end-goal of the research, typically expected to take the form of a
numerical model or a report paper. This is followed by a series of
attempts at getting closer to that final model or report by defining
and then verifying a series of intermediate hypotheses. In practice,
these hypotheses tend to take the form of informed guesses or open
questions and are explored and verified through a complex and often
messy process involing data summaries (aggregates), plots and also
definining and evaluating various numerical models. In a sense, this
kind of research can be likened to a search in the space of models where
the objective function is based on some form of model error. What is
important, working on each intermediate hypotheses results in creation
of numerous artifacts: data sets and summaries, plots, models, partial
reports, and other types of findings.

Such process is often of high intensity, with numerous alternative
questions that can be asked of the data, which potentially lead to
differing directions of research. For that reason, documenting the work
is both important and difficult. It is important because after exhausting
one direction of research, for the sake of revealing important insights,
one often decides to go back to the moment of fork and explore another
direction which at the time was less compelling. Thus, having that fork
well documented both in terms of data and code proves to be crucial.
On the other hand, documenting that kind of work is difficult, because
of the immense number of artifacts that need to be accounted for and
often, due to similarities, are confused for one another. Picking up a
trail left aside only temporarily can lead to a lot of wasted effort if
said trail was not identified correctly, and thus ensued in exploring
the wrong data or model.

This process, to some extent, resembles software development, and it
indeed includes software development as its part. The loop in which
hypotheses are formulated and verified bears some similarity to so-called
_"dev loop"_[^dev-loop] of writing actual code the test code, compiling
and verifying the correctness in a test run. Thus, bugs from the field of
software development have their counterpart in the field of data-related
research which take the form of errors in both code _and_ data.
The difference between these two processes, software development and
data-related research, is that in software development the new version
of the code replaces the old version and one rarely goes back to an old
version of the project in order to develop it in a whole new direction.
In data-related research, however, that happens farily often and thus
multiple version of the same model-building code are not supposed to
exclude but rather complement one another, as each highlights a different
yet valuable and interesting facet of the research problem.

[^dev-loop]: More about _dev loop_ here: https://leankit.com/learn/agile/agile-software-development/


Because of the intensity of the research process, the bookkeeping comes
with a considerable overhead and, thus, there is an incentive to limit
it only to the most important findings. That, on the other hand,
increases the risk of introducing mistakes into the process whenever
a backtracking occurs in the project, in the form of retrieving faulty
or sub-par model or model-building code or a malformed data set.
Each such mistake costs time and effort so a balance between actual
research work and the documentation-related overhead is eventually
established. What follows, is a brief attempt at characterizing typical
challenges related to documeting a research project and mistakes
involved in it.

## Maintaining the Data Pipeline

The first challenge is maintaing what can be collectively viewed as the
data pipeline designed and implemented during a data project. For the
purpose of this discussion it will be defined as the code: related to
extraction, transformation and loading (ETL) the data as well as
building, testing and scoring models, and a variety of plots and reports
which constitute the documentation. This part of a data project is the
one that overlaps with software development the most.

Its typical challenges are related to code development and refactoring
which are error-prone and can lead to misrepresentations in data and
erroneous decisions based on false reponses from ML models. Typically,
the answers to this challenge are found in the world of software
development: code versioning systems (CVS) are used to record chages in
code and persist data objects, often in their binary form. The main
disadvantage of that solution comes from the fact that CVS are designed
to aid in persisting source code, not (binary) objects like data sets,
and models, and thus lack in the area of discovering changes in those
binary objects. Also, source code tend to go well with CVS which store
their contents linearly, and analyze changes line-by-line. Data and
models, on the other hand, have an intrinsic, non-linear structure that
needs to be analyzed outside of a CVS.

Another importatnt difficulty is the fact that CVS will not record the
relationships between the data objects: for example, that a given model
comes from a given data set, and was created with a given version of the
source code. Software development is not troubled with information of
that nature and researchers are left to deal with that part of knowledge
building alone.


## Retrieving Past Versions of Data and Code

If the first challenge, described in the previous section, is related
to maintaining the present, then the second is definitely about
maintaining the past - or the history - of a data project. As it was
said earlier, it is typicall to keep a running list of open questions,
ordered according to their perceived importance to the project. Answering
those questions often involves looking back at some past state of work,
and retrieving an earlier version of data and code.

Given that CVS are often used to maintain the source code and data
objects (data sets and models), a natural first choice is to look at the
history of CVS. There are two possible problems involved in this need.
First is that the specific form of data or code had not been recorded as
a separate commit and only a slightly earlier or slightly later version
of said artifacts can be retrieved. Even if that is enough, and the
requested artifact can be rebuild by working off of what is available in
the history, there is still the need to invest extra effort and the risk
of introducing errors.

The second problem related to looking at the history in CVS is that the
artifacts might be identified mistakenly, and the following work based on
those artifacts might lead to false results, which is again a source of
potential waste of time and false conclusions.


## The third 

challenge: make the research reproducible, which means both the golden
path in the research, but also the broader question of explaining the
process and ideas in the context of alternative branches that were not
reported in the final report, e.g. branches that led to failed models
but were still important in terms of understanding the problem

------------



challenge: storing artifacts together with their multiple versions,
and figuring out the origin of any given artifact - is that plot related
to this or that version of the data I'm working on?
response: repository with the capability to mine expressions (commands)
and detect the actual structure of origin

challenge: making the research reproducible, which is derived from the
first challenge: documenting the results together with their accompanying
code and the actual source data; publishing enough details so that the
results can be verified by independent parties
response: include all relevant data and code in the research paper or
report; if the size of the data/source code prevents that, provide unique
identifiers (URLs) which grant access to relevant artifacts to parties
who are interested

challenge: turning artifacts into reports; this is a minor one, but can
be annoying


Since it is challenges on that
practical level that we seek to address with the solution described in
this paper.



As stated in Section 4, the three overarching goals for software packages
aiding research are: maintaining the state of the project state (code,
data, reports, findings) while in development, producing reports for
consumers of the findings, making the research reproducible. Let's
take a look at practical challenges related to this three goals.




Recognizing how important it is not to disrupt the existing workflows






loop: generate hypothesis, verify hypothesis

generates artifacts: data objects (data sets, models, summaries), code
(ETL functions, model formulas) and plots (but also tables, and others
visualizations, e.g. interactive)


as with software, one needs to plan where they want to go and document
the road so far, so that their deliverables constitue a body of work
intelligible for themselves as well as for third parties; which include:
peers (reviewers, scientific/academic papers), peers and superiors
(business reports), general public (blog posts)

we need to run all of our work against a cache memory, where the state
of the research is written to; it can be (as mentioned) a document,
an actual object repository or a bunch of files

without an adequate effort, the number of artifacts (and their versions)
can grow and push the complexity of the project beyond the level of
manageability; so the cache memory needs to be not only written to,
but also constantly re-organized, to constitute a representation of
what we know and think about the project

it is when wrong versions of data sets or code are being delivered
(sent over to production/implementation, reported to decision-makers,
etc.), reused (for similar projects, or if a report is required in
a cycle, e.g. monthly), or used a basis for further investigations
(e.g. grouping model is used to segment data, and then regression
models are built separately for each segment)

such mistakes in the long run are inevitable but the person conducting
the research should always strive to minimize the risk of making one

classification of mistakes: data (model), code (controller), plot (view)

data      | code      | plot      | example
correct   | incorrect | -         | ETL procedure is applied to a sample and then to the whole set
incorrect | correct   | -         | code is discarded, time is wasted
correct   | correct   | incorrect | good reults are discarded, wrong ETL code is used
... ... ...

view - decide to keep or discard code/data
data - report wrong results
code - apply incorrect procedure to another data set; stop other from reproducing results


# Problems

data and code run out-of-sync; having the repo amends that as data and code
for any given moment in history can be uniquely identified and retrieved from
the repo and relationships between artifacts written there; NICE!



these problems are human-specific, that is, they stem from out limited human
nature; there are various ways to classify them, but not really a formal way
to define them


identifying a single object - we know it's there, but we cannot find it;
maybe it was a part of a report (a few rows from a larger table), or there
is a plot that points to it (a table or a model), maybe there is come
characteristic of a model (AIC, R^2) but we cannot find the model that
actually matches it; in all of these cases being able to look at *all*
objects produced so far during the research project will give the definite
answer;
here we can be saving time (if the object is somewhere) or the reproducibility
itself (if for some reason we don't have it anymore)


verifying results so far - building a solution for a data-related problem
sometimes requires a number of iterations where various models (structure=formula
and kind lm vs. glm vs. tree) are generated, verified and finally compared;
those models need to be stored somewhere, be it RData files, RMarkdown document
or some other form of cache/storage;
because keeping record of all of those artifact can be confusing (?how? which
model did I write down, which are still in R's memory?) and introduce "a lot"
of extra overhead, we tend to limit what we write down only what we think will
be relevant in the end; having an intelligent cache that records everything
and then lets us pick and choose what we want to see and compare can be time-saving;
here we save time, most definitely, but also reduce the likelihood of errors
of omission, e.g. when part of the work disappears from the final report
because the user couldn't find it or forgot where it was stored


looking at the big picture - what have we learned so far, what do we think
and what do we know about the research project; what do we know about the
data, how do we want to approach the remainder of the work



# Repository - Model

 built
on top of a repository of R objects

store data, code and plots, record state of R session after each command,
together with all relevant details (e.g. packages and their versions, the
command itself); a.k.a "commit"

preserve the order of commits, effectively representing the whole history
of one's work

store objects under their SHA1, identify repetitions

allow to "go back in time" (retrieve a historical commit) and branch off
in that commit, thus preserving even more information about the
relationships between artifacts in the repository



# Repository vs. Problems

## Search for a Single Object

examples of questions that need to be answered and what kind of queries
return the requested information; a single object, a plot related to an
object, etc.

artifacts stored in R's global environment under the name of `m`

```{r eval=FALSE}
search(objects, named("m"))
```


artifacts that match some specific contents

```{r eval=FALSE}
search(objects, has_names("cylinder", "diameter"), has_rows(c(1, 5.5), c(10, 12.3)))
```




## Search for a Group of Objects

objects related to some shared source - e.g. various models derived from
the same original data set (directly or via other ETL-ed data sets); e.g.
inheriting from a given class, etc.


linear models built from input

```{r eval=FALSE}
search(objects, inherits("lm"), derived("input"))
```


any model derived from this specific data set

```{r eval=FALSE}
search(objects, is_model, derived(iris))
```


any model derived directly from this data set (no ETL expressions
that produce intermediate data set, unless they are a part of the
expression that produced that given model)

```{r eval=FALSE}
search(objects, inherits("lm"), derived(iris, direct = TRUE))
```


## Overview in Browser

when exploring (as opposed to "exploiting") the "search space", browser
can help build a high-level "map" of work done so far, various branches
in the timeline of the repository

logical grouping of objects - user can assign names/groups/branches to
subsets of the artifact tree and enrich the informations available to
himself (at a later time) or to his collaborators


also, when the id of the object is reported, and access to the object
store (repository) is granted, other researchers can investigate the
actual steps that led to creation of the said final result (object)



# Summary



# References

 * https://leankit.com/learn/agile/agile-software-development/
 * https://www.jvcasillas.com/post/2015-05-18_data_pipelines/
 * https://drsimonj.svbtle.com/a-tidy-model-pipeline-with-twidlr-and-broom
 * https://www.rstudio.com/resources/webinars/pipelines-for-data-analysis-in-r/
 * https://rstudio-pubs-static.s3.amazonaws.com/227736_57d977300b254865b95c256860d27209.html
 * https://cran.r-project.org/web/packages/pipeliner/README.html
 * https://cran.r-project.org/web/views/ReproducibleResearch.html
 * https://www.coursera.org/learn/reproducible-research
 * https://en.wikipedia.org/wiki/Reproducibility
 * https://www.r-bloggers.com/what-is-reproducible-research/

---
title: "Supporting Data Processing with an Object Repository"
author: "Lukasz A. Bartnik"
date: "`r Sys.Date()`"
output:
  rmarkdown::pdf_document:
    latex_engine: pdflatex
    pandoc_args: "--latex-engine-opt=--shell-escape"
header-includes:
   - \usepackage{svg}
vignette: >
  %\VignetteIndexEntry{Versioning Data Objects}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

<!--

Including SVG in PDF generated from RMarkdown:

  https://stackoverflow.com/questions/34064292/is-it-possible-to-include-svg-image-in-pdf-document-rendered-by-rmarkdown


-->


```{r setup, include=FALSE}
library(knitr)

knitr::opts_chunk$set(collapse = TRUE, comment = "#>", prompt = FALSE)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)
knitr::opts_chunk$set(fig.width=8, fig.height=6, fig.align = 'center')
knitr::opts_chunk$set(out.width='50%')
```


```{r message=FALSE}
library(readr)
library(lubridate)
library(magrittr)
library(dplyr)
library(ggplot2)
library(broom)
library(modelr)
library(forecast)
library(tidyr)
```

# Abstract

`experiment`, the package described in this white paper, is aimed at
reducing overhead of certain operations typical for data-oriented
research, especially as conducted in R. It keeps track of the history
of data and code changes across multiple R sessions and provides tools
to effectively work with that history: repeat and modify sequences of
R commands of arbitrary length and browse historical objects and code.
The next section gives an overview of the package design and the
conceptual model of data-oriented reserach which it is based on.
Section 3 follows-up with a sample R session and explains how
`experiment` can be used to reduce certain types of operational
overhead.


# The Basic Model of Data Research

`experiment` has been designed with a certain model of __data research__
in mind which in this paper means any activity where one or more data
set are cleaned, transformed and possibly modelled. During that process
numerous __artifacts__ are created, inspected and organized into a
knowledge base. Those artifacts include but are not limited to: plots,
models, data sets, code, summaries and printouts. A __data project__
is a time- and scope-limited instance of data research.


`experiment` assumes a certain model of data research performed by means
of R. __R session__ is both an artifact container and an interface to
interact with those artifacts. `experiment` keeps track of all changes
made to artifacts stored in R session and upon each change records a
snapshot of all objects stored in R session, namely in the
__global environment__, the most recent __R expression__, the plot or
console printout, should either of them be modified, and a pointer to
the previous (parent) snapshot.


In the `experiment` package such snapshot is called a `commit` and the
tree of commits organizes the history of the data project in the
__time view__, ordered according to their parent-child relationship.
Although it is possible to view the graph of commits and restore any
recorded state of R session, such level of detail becomes confusing if
the historical data is not recent enough. Otherwise, more appropriate is
the __data view__, which organizes artifacts according to their origin.
Both views can be better understood and compared by looking at the
following sample R session.

```{r eval=FALSE}
x <- 1
z <- 100
y <- x + 2
z <- z + x * y
```

This sequence of commands results in 4 commits:

1. `(x = 1)`
1. `(x = 1, z = 100)`
1. `(x = 1, y = 3, z = 100)`
1. `(x = 1, y = 3, z = 103)`.

When viewed in the __time view__, they form a linear graph shown in
figure 1a. Conversely, the __data view__ is shown in figure 1b. 

\begin{figure}[h]
\centering
\includesvg[width=400pt]{plots/01-view}
\caption{The __time view__ (a) and the __data view__ (b) of a sample R session.}
\end{figure}


A sequence of commits is called a __branch__. Since `experiment` allows
the user to go back to any of the recorded commits and continue their
work from there, there can be multiple branches in the __time view__.


# Local and Global Phases of Data Research

Given the basic model of data research with `experiment` we can introduce
the local and the global phases of data research. Most data projects go
through multiple transitions between these phases or modes of work. The
__global__ phase centers around building a narrative about the research
done so far which combines outcomes of multiple local iterations. It is
also where specific local questions are formulated and prioritized. The
__local__ phase is aimed at answering a specific question about the data
and is often a trial-and-error process which establishes the desired order
and kind of data transformations. It is concluded when enough new insight
is gained to pose new questions and initiate a subsequent local phase.

In the context of R, the local is focused on a sequence of R commands
which form a __data pipeline__. That sequence is iteratively improved,
expanded and debugged, until a set of artifacts that answer the question
at hand is successfully produced. Conversely, during the global phase
the notable findings are preserved in a variety of ways (R scripts, text
and binary data files, plots and reports).


Here maybe about __session cache__ and __project repository__ and how
time view and data view are designed for them?


The solution described in this
white paper is aimed at reducing the time and effort overhead of either
of these phases and of transitioning between them.


# Basic notions


* `data project`
* `R session`
* `tracker`
* `session cache` (cache)
* `project repository` (repository)
* `artifact`
* `commit`
* `branch`
* `time view`
* `data view`

# Example

## The Local Phase

Iterate over a single idea, fix errors in the expression, generate a number
of versions of the object and plots, finally choose objects that should make
it to the persistent storage.


### Iterate over a model

Here is a very simple example of what might happen during the initial
phase of a data project. We load the data, build a few simple models
and create a few plots to get the initial insights.


```{r, message=FALSE,warning=FALSE}
file <- 'site_10.csv'
data <- read_csv(file)
```


Let's look at the whole data, one year worth of hourly readings.

```{r, message=FALSE,warning=FALSE}
ggplot(data, aes(x = time, y = usage)) + geom_line()
```


Let's start with monthly averages.

```{r, message=FALSE,warning=FALSE}
m <- data %>%
  mutate(month = as.factor(month(time, label = TRUE))) %>%
  {lm(usage ~ month, .)}
glance(m)
```



There seems to be some regularity in the data.
Let's see if there average usage depends on the day of the week.

```{r, message=FALSE,warning=FALSE}
m <- data %>%
  mutate(wday = wday(time, label = TRUE)) %>%
  {lm(usage ~ wday, .)}
glance(m)
tidy(m)
```


Now let's look at the hourly profile.

```{r, message=FALSE,warning=FALSE}
m <- data %>%
  mutate(hour = as.factor(hour(time))) %>%
  {lm(usage ~ hour, .)}
glance(m)
```

Interesting, both day of the week and the hour of the day can be used
to model the usage. Let's see what happens when we combine them.


```{r, message=FALSE,warning=FALSE}
data %>%
  group_by(hour = hour(time), wday = wday(time, label = TRUE)) %>%
  summarise(usage = mean(usage, na.rm = TRUE)) %>%
  ggplot(aes(x = hour, y = usage)) + geom_point() + facet_wrap(~wday)
```


And now a quantitative assessment of the same model.

```{r, message=FALSE,warning=FALSE}
m <-
  data %>%
  mutate(hour = as.factor(hour(time)), wday = wday(time, label = TRUE)) %>%
  {lm(usage ~ hour:wday, .)}
glance(m)
```

Finally, let's combine all three input features of our model.

```{r, message=FALSE,warning=FALSE}
m <-
  data %>%
  mutate(hour = as.factor(hour(time)), wday = wday(time, label = TRUE),
         month = as.factor(month(time))) %>%
  {lm(usage ~ month + hour:wday, .)}
glance(m)
```



### Browse the history

Up to this point the session cache collected a number of artifacts.
Let's see the summary of the session cache.

```{r eval=FALSE}
tracker
#> Tracker :  ON
#> Branch  :  "default"
#> Commits :  15 in branch, 15 total
```


This is what the history graph looks like. First, in the dimension of
time, that is, where nodes are connected according to the order in
which they were created.

\begin{figure}[h]
\centering
\includesvg[width=140pt]{plot-1}
\caption{svg image}
\end{figure}


Another way to look at the history is the dimension of origin, that is,
the parent-child relationship between artifacts.

\begin{figure}[h]
\centering
\includesvg[width=400pt]{plot-2}
\caption{svg image}
\end{figure}


The information depicted in the first graph can be accessed in R session
via the `tracker$history` facility. The example below follows up on our
sample R session. Commits recorded in the session repository are printed
out starting with the oldest. The most recent commit is assigned index
`a`, the one before is assigned index `b` and so on. Indices can be used
to point to a specific step in the history. For each step we print out
the list of artifact names and the expression abbreviated to one line.

In actual R session variables which are introduced or replaced are printed
in green.


```{r eval=FALSE}
tracker$history(n = 50)
#> o: file
#>    file <- 'site_10.csv'
#> n: data file
#>    data <- read_csv(file)
#> m: data file [plot]
#>    ggplot(data, aes(x = time, y = usage)) + geom_line()
#> l: data file m
#>    m <- data %>% mutate(month = as.factor(month(time, ...
#> k: data file m [printout]
#>    glance(m)
#> j: data file m
#>    m <- data %>% mutate(wday = wday(time, label = TRUE)) ...
#> i: data file m [printout]
#>    glance(m)
#> h: data file m [printout]
#>    tidy(m)
#> g: data file m
#>    m <- data %>% mutate(hour = as.factor(hour(time))) ...
#> f: data file m [printout]
#>    glance(m)
#> e: data file m [plot]
#>    data %>% group_by(hour = hour(time), wday = wday(time, ...
#> d: data file m
#>    m <- data %>% mutate(hour = as.factor(hour(time)), ...
#> c: data file m [printout]
#>    glance(m)
#> b: data file m
#>    m <- data %>% mutate(hour = as.factor(hour(time)), ...
#> a: data file m [printout]
#>    glance(m)
```


Each commit can be printed in more detail. This is where the letters-indices
come in handy.

```{r eval=FALSE}
tracker$history$b
#> Commit afb67ad6
#> data :  data.frame[8784, 2]
#> file :  character(1)
#> m    :  lm(formula = usage ~ month + hour:wday, data = .)
#>
#>   m <-
#>    data %>%
#>    mutate(hour = as.factor(hour(time)), wday = wday(time, label = TRUE),
#>           month = as.factor(month(time))) %>%
#>    {lm(usage ~ month + hour:wday, .)}
```


Since we have found an interesting artifact, we can now instruct `tracker`
to store it in the persistent repository of artifacts. We point to one
object but the whole sequence of objects required to create it is copied
from the session repository to the persistent one. If any of the objects
is already present in the persistent repository (`session cache` vs `repository`)
the ends are connected and a consistent graph of artifacts is maintained.



```{r eval=FALSE}
tracker$history$b$m %>% persist
#> Persisted 3 objects: m, data, file
```


Let's all persist the final profile plot.

```{r eval=FALSE}
tracker$history$e$`[plot]` %>% persist
#> Persisted 1 plot.
```



## Repeat the Sequence

Another useful tool is the `replay` function. It accepts a list of objects
that were changed, finds their originals, and re-computes all nodes descendant
in the `data` view. In our example we replace the data file name and then
ask R to recalculate all object below it. It creates a new branch in the
session cache and names it `"replay(file) #1"`.


```{r eval=FALSE}
file <- 'site_88.csv'
replay(file)
#> Objects changed: file
#> Recalculating: 6 objects, 2 plots, and 6 printouts.
#> Branch name: "replay(file) #1"
```


Let's look at the state of the tracker.

```{r eval=FALSE}
tracker
#> Tracker :  ON
#> Branch  :  "replay(file) #1"
#> Commits :  15 in branch, 30 total
```


The current R session contains a new model, built with the same pipeline
but for a different data set. Let's take a look at the model and compare
it to the one already stored in the project repository. The new model:

```{r eval=FALSE}
glance(m)
```
```{r echo=FALSE}
file <- 'site_88.csv'
data <- read_csv(file)
m2 <-
  data %>%
  mutate(hour = as.factor(hour(time)), wday = wday(time, label = TRUE),
         month = as.factor(month(time))) %>%
  {lm(usage ~ month + hour:wday, .)}
glance(m2)
```

The model built for __site 10__:

```{r eval=FALSE}
glance(tracker$repository$m)
```
```{r echo=FALSE}
glance(m)
```

Let's also take a look at the final profile plot for __site 88__. In the
session history it can be accessed via:

```{r eval=FALSE}
tracker$history$e$`[plot]`
```
```{r echo=FALSE,message=FALSE,warning=FALSE}
data %>%
  group_by(hour = hour(time), wday = wday(time, label = TRUE)) %>%
  summarise(usage = mean(usage, na.rm = TRUE)) %>%
  ggplot(aes(x = hour, y = usage)) + geom_point() + facet_wrap(~wday)
```

Look back, change input, generate two new models for two other buildings;
use `repeat`


Browse new branches, choose the two new models and store them persistently


## Browse the Tree of Understanding

Look at the branches stored in the persistent store, this time presented
according to their origin



